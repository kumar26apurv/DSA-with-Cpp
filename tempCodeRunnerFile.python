import cv2
import mediapipe as mp
class handDetector():
    def _init_(self, mode=False, maxHands=2, detectionCon=0.5,modelComplexity=1,trackCon=0.5):
        self.mode = mode
        self.maxHands = maxHands
        self.detectionCon = detectionCon
        self.modelComplex = modelComplexity
        self.trackCon = trackCon
        self.mpHands = mp.solutions.hands
        self.hands = self.mpHands.Hands(self.mode, self.maxHands,self.modelComplex,
                                        self.detectionCon, self.trackCon)
        self.mpDraw = mp.solutions.drawing_utils

    def findHands(self, image, draw=True):
            imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            self.results = self.hands.process(imageRGB)

            if self.results.multi_hand_landmarks:
                for handLms in self.results.multi_hand_landmarks:
                    if draw:
                        self.mpDraw.draw_landmarks(image, handLms, self.mpHands.HAND_CONNECTIONS)
            return image

    def findPosition(self, image, handNo=0, draw=True):
            lmlist = []
            if self.results.multi_hand_landmarks:
                Hand = self.results.multi_hand_landmarks[handNo]
                for id, lm in enumerate(Hand.landmark):
                    h, w, c = image.shape
                    cx, cy = int(lm.x * w), int(lm.y * h)
                    lmlist.append([id, cx, cy])
                    if draw:
                        cv2.circle(image, (cx, cy), 10, (255, 255, 0), cv2.FILLED)

            return lmlist

def main():
        cap = cv2.VideoCapture(0)
        tracker = handDetector()

        while True:
                success, image = cap.read()
                image = tracker.findHands(image)
                lmList = tracker.findPosition(image)
                if len(lmList) != 0:
                    print(lmList[4])

                cv2.imshow("Video", image)
                cv2.waitKey(1)
if _name_ == "_main_":
 main()
 import cv2
import time
import numpy as np
import handtrackmodule as htm
import math
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume

wCam,hCam=1080,1080

cap=cv2.VideoCapture(0)
cap.set(3,wCam)
cap.set(4,hCam)
pTime=0

detector=htm.handDetector(detectionCon=0.8)

devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(
    IAudioEndpointVolume.iid, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))
#volume.GetMute()
#volume.GetMasterVolumeLevel()
volRange=volume.GetVolumeRange()

minVol=volRange[0]
maxVol=volRange[1]
vol=0
volBar=400
volPer=0

while True:
    success,img=cap.read()
    img=detector.findHands(img)
    lmList=detector.findPosition(img,draw=False)
    if len(lmList)!=0:
        #print(lmList[4],lmList[8])

        x1,y1=lmList[4][1],lmList[4][2]
        x2,y2=lmList[8][1], lmList[8][2]
        cx,cy=(x1+x2)//2,(y1+y2)//2

        cv2.circle(img,(x1,y1),15,(255,255,0),cv2.FILLED)
        cv2.circle(img,(x2,y2),15,(255,255,0),cv2.FILLED)
        cv2.line(img,(x1,y1),(x2,y2),(255,255,0),3)
        cv2.circle(img,(cx,cy),15,(255,255,0),cv2.FILLED)

        length=math.hypot(x2-x1,y2-y1)
        #print(length)
        vol=np.interp(length,[50,300],[minVol,maxVol])
        volBar=np.interp(length,[50,300],[400,150])
        volPer = np.interp(length, [50, 300], [0, 100])
        print(int(length),vol)
        volume.SetMasterVolumeLevel(vol, None)

        if length<50:
            cv2.circle(img, (cx, cy), 15, (0, 255,255), cv2.FILLED)

    cv2.rectangle(img,(50,150),(85,400),(0,255,0),3)
    cv2.rectangle(img, (50,int(volBar)), (85, 400), (255, 255, 0), cv2.FILLED)
    cv2.putText(img,f'{int(volPer)} % ',(40,450),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),4)

    cTime = time.time()
    fps = 1 / (cTime - pTime)
    pTime = cTime

    cv2.putText(img,f'FPS:{int(fps)}',(40,50),cv2.FONT_HERSHEY_SIMPLEX,2,(255,255,0),6)
    cv2.imshow("Image",img)
    if cv2.waitKey(1) & 0xFF==ord('s'):
        break
cv2.destroyAllWindows()